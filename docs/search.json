[
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "3  Week 2 - Xaringan",
    "section": "",
    "text": "github link: https://github.com/zzzzzoy/RS_xaringanqu"
  },
  {
    "objectID": "week4.html",
    "href": "week4.html",
    "title": "5  Week 4 - Policy",
    "section": "",
    "text": "5.0.1 Summary\nAhmedabad is situated in the largest district of the western Indian state of Gujarat, with a population of 7.2 million(Knowlton et al., 2014). Its rapid growth in sectors such as real estate, automotive, and pharmaceuticals has positioned it among the top ten fastest-growing cities in India, poised to become one of the country’s major metropolises by 2020(Knowlton et al., 2014). However, Ahmedabad faces significant challenges due to extreme heat.\nIn 2010, Ahmedabad experienced its most severe heat wave on record, with temperatures soaring to 46.8°C (116°F), resulting in 1,344 additional deaths registered. This event served as a warning. The 2016 Heat Action Plan is an updated version of the first comprehensive early warning system and preparedness plan launched in Ahmedabad in 2013. The Heat Action Plan aims to implement four key strategies:\n\nBuilding Public Awareness and Community Outreach to communicate the risks of heat waves and implement practices to prevent heat-related deaths and illnesses.\nInitiating an Early Warning System and Inter-Agency Coordination to alert residents of predicted high and extreme temperatures.\nCapacity Building Among Health Care Professionals to recognize and respond to heat-related illnesses, particularly during extreme heat events.\nReducing Heat Exposure and Promoting Adaptive Measures by mapping high-risk areas of the city, increasing outreach and communication on prevention methods, and providing access to potable drinking water and cooling spaces during extreme heat days. Collaboration with non-governmental organizations is also identified as a means to expand outreach and communication with the city’s most at-risk communities.\n\n\n\n5.0.2 Application\nA remotely sensed data set that could be used to assist in mapping high-risk areas of the city is high-resolution satellite imagery such as Landsat satellite imagery coupled with geographic information system (GIS) data.\nHigh-resolution satellite imagery provides detailed visual information about land cover, land use, and environmental conditions within the city. By analyzing this imagery, it is possible to identify characteristics associated with high-risk areas such as urban heat islands, areas with limited green spaces, industrial zones, and densely populated neighborhoods.\nGIS data, on the other hand, includes various layers of spatial information such as demographic data, land use data, infrastructure data, and environmental data. By integrating satellite imagery with GIS data, it becomes possible to overlay and analyze different spatial layers to identify and map high-risk areas.\nHow the data setes be used: 1. Land Use/Land Cove(LULC) classification is a crucial step. Landsat imagery can be used to classify land cover types within the city, such as impervious surfaces (roads, buildings), vegetation, water bodies, and bare soil. Areas with high levels of impervious surfaces (e.g., urban areas) can be pinpointed, which are more prone to heat retention and urban heat island effects. 2. Landsat imagery can also be used to derive surface temperature maps of the city. These temperature maps can help in identifying hotspots within the city that are at higher risk during heatwaves. 3. Landsat imagery can provide insights into the health and density of vegetation within the city. By mapping vegetation health, policymakers can identify areas where increasing green infrastructure could reduce heat-related risks.\nThis approach aligns with several global agendas and goals:\nSustainable Development Goals (SDGs): The United Nations’ SDGs aim to address global challenges such as poverty, inequality, climate change, and environmental degradation. By mapping high-risk areas of the city, policymakers can identify areas where vulnerable populations are disproportionately affected by environmental risks and take targeted actions to address these issues, contributing to SDGs such as Goal 11 (Sustainable Cities and Communities) and Goal 13 (Climate Action).\nHow it advances current local, national or global approaches:\n\nProviding spatially explicit information to support evidence-based decision-making and resource allocation.\nIdentifying areas where interventions are most needed to protect vulnerable populations and improve public health outcomes.\nEnhancing the effectiveness and efficiency of urban planning, infrastructure development, and disaster risk reduction efforts.\nStrengthening resilience to climate change and extreme weather events at the local, national, and global levels.\n\n\n\n5.0.3 Reflection\nAhmedabad, as a rapidly growing urban center facing significant heat-related challenges, serves as a compelling case study. The city’s population density, urban expansion, and environmental conditions contribute to heightened risks during heatwaves. Understanding these local dynamics is crucial for tailoring effective interventions and resilience-building strategies. The utilization of remotely sensed data, such as Landsat satellite imagery, underscores the importance of leveraging technological advancements for informed decision-making. By analyzing land cover, temperature distribution, policymakers could gain valuable insights into the spatial distribution of risk within the city. This enables targeted interventions and resource allocation to areas most in need."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "learning diary",
    "section": "",
    "text": "1 Introduction\nHi, I’m Zhongzhen. In my undergraduate studies, I majored in mathematics. During that time, most of my coursework delved into pure theoretical knowledge, and I took numerous courses heavily focused on abstract mathematical concepts. However, I found limited opportunities to apply what I had learned to real-world scenarios.\nDiscovering fields like geographic information systems (GIS) and remote sensing has been incredibly rewarding for me. Casa and remote sensing offer abundant opportunities to bridge theoretical knowledge with practical applications.\nMy interest lies in optimizing spatial problems. This passion stems from my undergraduate fascination with operational research, where I delved deep into the study of optimization techniques. Even I once considered pursuing operational research in my graduate studies."
  },
  {
    "objectID": "week1.html",
    "href": "week1.html",
    "title": "2  Week 1 - Introduction to Remote Sensing",
    "section": "",
    "text": "2.0.1 Summary\n\nRemote sensing is defined as acquiring information from a distance. Remote sensing can collect data without physical contact with objects by sensors.\nPassive or active sensors. Active sensors have an energy source and will actively emit electromagnetic energy and then receive\nElectromagnetic energy\n\nWavelength: long wavelength = low frequency = low energy\nElectromagnetic spectrum: total range of wavelengths of EM radiant\nEnergy interaction: reflected, absorbed by the surface, transmitted through the surface, scattered by particles in the atmosphere.\nBlue light shorter wavelength and more easily scatters(450nm-blue(shortest visible light) 550nm-green 700nm-red(longest)).\nThe spectral reflectance characteristics of surface materials are different. That is why sensors can identify materials. In the visible spectrum, chlorophyll in plant leaves strongly absorbs light in the blue and red regions, but reflects green light. This is why healthy vegetation appears green to our eyes.\nThe sun is the primary source of EM energy\n\nFour resolutions\n\nSpatial resolution the size of the raster grid per pixel\nSpectral resolution the number of bands sensor records data\nRadiometric resolution identify differences in light or reflectance, in practice this is the range of possible values.\nTemporal resolution the time it revisits\n\n\n\n\n2.0.2 Application\n\n2.0.2.1 Sentinel-2\n\nSpatial resolution of 10 m, 20 m and 60 m\nRevisiting every 10 days\nMulti-spectral data with 13 bands in the visible, near-infrared, and short-wave infrared part of the spectrum\n\n\nApplication\n\n\nMonitoring inland water bodies.\n\n\n\nWatching coastal waters.\n\n\n\n\n2.0.2.2 Landset\nThe below figures are my practical output\ngreen rectangle = bare earth pink rectangle = vegetation purple rectangle = urban area red rectangle = water\n\nIt seems that the 7 bands.tif file is too large for my computer. I only export GeoTIFFS with bands 2,3,4.\nspectral profiles\n\ndensity plot\n\n\n\n\n2.0.3 reflection\nThis is the very beginning of remote sensing, for me, it is the first contact for me with the subject. Almost everything is new for me. The only thing that I am familiar with is electromagnetic waves. Different wavelength gives electromagnetic waves different properties, like the penetrative ability. I thought longer wavelengths were easier to penetrate, while shorter wavelengths were more difficult. However, longer wavelengths of electromagnetic radiation, such as infrared and microwaves, tend to penetrate certain substances more easily. This is because some materials have lower absorption rates for longer wavelengths, allowing radiation of these wavelengths to penetrate relatively easily.\nFor other substances and specific wavelength ranges, the situation may be different. For example, some materials have high absorption rates for certain wavelengths of electromagnetic radiation, making it difficult for radiation of these wavelengths to penetrate the material. This is common in the visible light range, as many substances have high absorption rates for visible light, making it difficult for visible light to penetrate them. That is how the sensor identifies different materials. Also, remote sensing sensors can capture electromagnetic signals reflected or emitted from the Earth’s surface and divide them into multiple different bands. For example, a sensor for visible light and infrared might be divided into several different bands, each corresponding to a specific part of the visible light or infrared spectrum. Different bands capture information about different types of features on the Earth’s surface, so the choice of bands is crucial for the success of specific applications. That’s why when I do practicals there are so many bands that make me confused initially.\nAnother question I met that could take much time for me to figure out is what are the differences between satellites. I know they have different resolutions and bands, but what do the differences refer to, what advantages do they have, and Why are so many types of satellites needed?"
  },
  {
    "objectID": "week3.html",
    "href": "week3.html",
    "title": "4  Week 3 - Remote sensing data",
    "section": "",
    "text": "4.0.1 Summary\n\n4.0.1.1 Corrections\n\n4.0.1.1.1 Geometric\n\nFour reasons make image distortions:\n\n\nView angle\nTopography\nWind\nRotation of the earth (from satellite)\n\n\nSolution:\n\n\nseveral algorithms(linear regression, Helmert transformation,Polynomial algorithms 1-3, Thin Plate Spline (TPS), Projective transformation)\ndecide which model to be used(The model with the lowest RMSE will fit best)\nre-sample the final raster\n\n\n\n4.0.1.1.2 Atmospheric\n\nSources:\n\n\nAtmospheric scattering\nTopographic attenuation\n\n\nMethod:\n\n\nEmpirical Methods\nDark Object Subtraction\nPsuedo-invariant Features (PIFs)\nAtmospheric radiative transfer models\n\n\n\n4.0.1.1.3 Orthorectification / Topographic correction\n\nCosine correction\nMinnaert correction\nStatistical Empirical correction\nC Correction (advancing the Cosine)\n\n\n\n4.0.1.1.4 Radiometric Calibration\n\nRadiometric Calibration = Digital Number to spectral radiance(true spectral radiance on earth surfaces is different with spectral radiance that sensors capture)\nSensor calibration: We use measurements to adjust\nReflectance: A ratio comparing the quantity of light emitted by a target to the quantity of light incident upon it.\n\n\nshould be considered in radiance correction\n\n\n\n\n4.0.1.2 Joining data sets/Enhancements\n\n4.0.1.2.1 Joining data sets\n\nIf one image is not sufficient to cover the area we wish to study, we need to merge several images.\nis called ‘Mosaicking’ in remote sensing\nfeather images together\n\n\n\n4.0.1.2.2 Enhancements\n\nRatio\n\n\nto identify a certain landscape feature\nprinciple of NDVI: vegetation reflects more in the NIR but absorbs in the Red wavelength\n\n\nFiltering\nTexture\nData fusion\nPCA\n\n\n\n\n\n4.0.2 Application\nDue to its extensive history, simplicity, and utilization of readily accessible multi-spectral bands, the NDVI has emerged as the predominant index employed for evaluating vegetation(Huang et al., 2021). The article ‘Application of Normalized Difference Vegetation Index (NDVI) for the Detection of Extreme Precipitation Change’(Pei, Zhou and Xia, 2021) shows the application of NDVI in detecting extreme precipitation. NDVI can reflect extreme precipitation events because such events impact vegetation activity, potentially leading to lush vegetation and consequently higher NDVI values. By observing changes in NDVI values, one can assess variations in extreme precipitation. This study analyzed the application of minimum, average, and maximum NDVI in detecting extreme precipitation changes, using the middle and lower reaches of the Yangtze River as a case study. In this region, the location with the highest NDVI value represents the most vigorous vegetation growth. The study compared the performance of these three NDVI indicators in responding to extreme precipitation changes. They found that the maximum NDVI is more suitable for capturing the response of vegetation activity to extreme precipitation events. From my understanding, their approach involves setting a threshold to determine extreme precipitation events and then conducting separate correlation analyses between the three NDVI indicators and extreme precipitation intensity and frequency. The indicator with the highest correlation value is better at reflecting extreme precipitation events. Interestingly, the maximum NDVI value showed a negative correlation with extreme precipitation intensity and frequency, while the other two showed positive correlations. This implies that intense precipitation is detrimental to the growth of tall and lush vegetation. I speculate that intense precipitation may be accompanied by strong winds or other extreme weather conditions, which could potentially damage tall and lush vegetation.\n\n\n4.0.3 Reflection\nThis week’s lesson on correction seems very similar to what we previously learned about data cleaning. Satellite images often have imperfections, and you need methods to correct them. Just like dealing with data in GIS and FSDS, you may need to remove NA values and handle outliers. Mosaicking is akin to merging or joining datasets, while enhancement involves processing the data to extract desired information, such as mean, extremes, and density. NDVI can highlight vegetation, and PCA can reduce dimensionality.\nThe article mentioned in my application made me realize that NDVI, besides analyzing vegetation information, can also be used to analyze factors related to vegetation changes, such as precipitation. This has broadened my perspective significantly."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abdikan, S. et al. (2016) ‘LAND COVER MAPPING USING SENTINEL-1 SAR DATA’, The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, XLI-B7, pp. 757–761. Available at: https://doi.org/10.5194/isprs-archives-XLI-B7-757-2016.\nBelgiu, M. and Drăguţ, L. (2016) ‘Random forest in remote sensing: A review of applications and future directions’, ISPRS Journal of Photogrammetry and Remote Sensing, 114, pp. 24–31. Available at: https://doi.org/10.1016/j.isprsjprs.2016.01.011.\nChaturvedi, S.K., Banerjee, S. and Lele, S. (2020) ‘An assessment of oil spill detection using Sentinel 1 SAR-C images’, Journal of Ocean Engineering and Science, 5(2), pp. 116–135. Available at: https://doi.org/10.1016/j.joes.2019.09.004.\nFirst applications from Sentinel-2A (no date). Available at: https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-2/First_applications_from_Sentinel-2A (Accessed: 17 March 2024).\nGorelick, N. et al. (2017) ‘Google Earth Engine: Planetary-scale geospatial analysis for everyone’, Big Remotely Sensed Data: tools, applications and experiences, 202, pp. 18–27. Available at: https://doi.org/10.1016/j.rse.2017.06.031.\nHansen, M.C. et al. (2013) ‘High-Resolution Global Maps of 21st-Century Forest Cover Change’, **Science*, 342(6160), pp. 850–853. Available at: https://doi.org/10.1126/science.1244693.\nHuang, S. et al. (2021) ‘A commentary review on the use of normalized difference vegetation index (NDVI) in the era of popular remote sensing’, Journal of Forestry Research, 32(1), pp. 1–6. Available at: https://doi.org/10.1007/s11676-020-01155-1.\nKnowlton, K. et al. (2014) ‘Development and implementation of South Asia’s first heat-health action plan in Ahmedabad (Gujarat, India)’, International journal of environmental research and public health, 11(4), pp. 3473–3492.\nM. Amani et al. (2020) ‘Google Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review’, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 13, pp. 5326–5350. Available at: https://doi.org/10.1109/JSTARS.2020.3021052.\nPal, M. and Mather, P.M. (2005) ‘Support vector machines for classification in remote sensing’, International Journal of Remote Sensing, 26(5), pp. 1007–1011. Available at: https://doi.org/10.1080/01431160512331314083.\nPei, F., Zhou, Y. and Xia, Y. (2021) ‘Application of Normalized Difference Vegetation Index (NDVI) for the Detection of Extreme Precipitation Change’, Forests, 12(5). Available at: https://doi.org/10.3390/f12050594.\nPlatform – Google Earth Engine (no date). Available at: https://earthengine.google.com/platform/ (Accessed: 19 March 2024).\nSentinel-1 - Missions - Sentinel Online - Sentinel Online (no date). Available at: https://sentinels.copernicus.eu/web/sentinel/missions/sentinel-1 (Accessed: 17 March 2024).\nSustainable Development Goals | United Nations Development Programme (no date). Available at: https://www.undp.org/sustainable-development-goals (Accessed: 21 March 2024).\nThornton, M.W., Atkinson, P.M. and Holland, D.A.(2006) ‘Sub‐pixel mapping of rural land cover objects from fine spatial resolution satellite sensor imagery using super‐resolution pixel‐swapping’, International Journal of Remote Sensing, 27(3), pp. 473–491. Available at: https://doi.org/10.1080/01431160500207088.\nYu, Q. et al. (2006) ‘Object-based detailed vegetation classification with airborne high spatial resolution remote sensing imagery’, Photogrammetric Engineering & Remote Sensing, 72(7), pp. 799–811."
  },
  {
    "objectID": "week5.html",
    "href": "week5.html",
    "title": "6  Week 5 - Google Earth Engine",
    "section": "",
    "text": "6.0.1 Summary\nCloud Computing Platform for planetary-scale geospatial analysis\n\n6.0.1.1 Terms\n\nmage = raster\nFeature = vector\nImage stack= ImageCollection\nFeature stack (lots of polygons) = FeatureColletion\nImage scale = pixel resolution\n\n\n\n6.0.1.2 Language - javascript\n\nWebsite programming language\nsimilar to Python and R\ndefine objects with var\ne.g\nvar object = {string: \"hello world\", int: 1, float: 1.1};\nprint('Print string:', object['string']);\nvar x = 10;\nprint(x)\n\n\n\n6.0.1.3 Server side\n\nSome codes that run on the server side\nGEE Objects = starting with ee stored on the server\nOverview of GEE functions \n\n\n\n6.0.1.4 Advantages and limitations\n\n\n\n6.0.1.5 Overview of GEE Code Editor\n\n\n(Source: Google Earth Engine)\n\n\n\n\n6.0.2 Application\n\nThis is a summary of the applications of GEE \n\n(Source: M. Amani et al., 2020)\n\n\nA specific example of an application is that Hansen et al. (2013) utilized decision trees generated from extensive training data and a deep stack of metrics computed from Landsat scenes to characterize forest extent, loss, and gain from 2000 to 2012(Gorelick et al., 2017). The article ‘High-Resolution Global Maps of 21st-Century Forest Cover Change’(Hansen et al., 2013) investigates global forest cover change from 2000 to 2012 using data with a spatial resolution of 30 meters. This high-resolution global study represents a significant advancement over previous studies conducted at coarser scales. The study quantifies forest loss and gain using specific methodologies and provides annual loss information. This detailed quantification offers more accurate data, facilitating a deeper understanding of forest change trends. By employing spatially explicit methods, the study results are more precise and interpretable in spatial terms. This enhances the ability to accurately locate and analyze factors and patterns influencing forest cover change. GEE facilitated the processing of large-scale Landsat imagery datasets, allowing for efficient computation of metrics and decision tree generation. This enabled the researchers to analyze forest cover change globally over a significant period. GEE’s capabilities were leveraged to conduct spatially explicit analyses, enabling the researchers to map forest cover extent, loss, and gain at a high spatial resolution of 30 meters. This spatial specificity provided detailed insights into global forest dynamics.\n\n\n6.0.3 Reflection\nThis week, we learned a new tool for research called Google Earth Engine, which allows us to run code on servers. During the practical exercises, I noticed a significant increase in speed compared to running code locally, especially when compared to tools like SNAP, which felt quite slow. Tasks such as Mosaic images, Clip images, Texture measures, and PCA, which we previously learned, can all be completed on Google Earth Engine’s servers. The only difference is that Google Earth Engine uses a different programming language, JavaScript, which I wasn’t very familiar with at the beginning. However, as I followed along with the practical exercises, I found that its logic is similar to Python and R, which made it easier for me to pick up. As a result, I became more and more proficient with it as I progressed through the exercises."
  },
  {
    "objectID": "week6.html",
    "href": "week6.html",
    "title": "7  Week 6 - classification 1",
    "section": "",
    "text": "7.0.1 Summary\nThis week we learned the application of machine learning in classification in remote sensing.\n\n7.0.1.1 Classification and Regression Trees (CART)\n\nuses a tree structure with nodes representing features, branches indicating decisions based on feature values, and leaf nodes showing predicted class labels (for classification) or numerical values (for regression)\nrecursively splits the feature space to maximize homogeneity within subsets, using measures like Gini impurity (for classification) or lowest SSR (for regression)\nallows controlling tree complexity with parameters like maximum depth and minimum samples per node, preventing overfitting and improving generalization.\n\n\n\n7.0.1.2 Random Forest\n\na machine learning algorithm based on ensemble learning. It consists of multiple decision trees, where each tree is a weak learner\nintroduces randomness to improve model generalization. It uses random sampling and random feature selection when building each decision tree, resulting in diversity among the trees(never all of them)\nemploys Bagging (Bootstrap Aggregating) to construct each decision tree. It creates multiple training sets by sampling from the original dataset with replacement and builds a decision tree on each training set\nWhen constructing each decision tree, Random Forest randomly selects a subset of features to consider for splitting nodes, rather than using all features. This helps reduce feature correlation and increases model diversity\ndetermination of the prediction: the class with the most votes is selected as the final prediction for classification tasks, while for regression tasks, the average prediction of all trees is taken.\n\n\n\n7.0.1.3 Support Vector Machine (SVM) - Supervised classification\n\nlinear binary classifier - like logistic regression\nMaximum margin between two classes of training data = maximum margin classifier\nPoints on the boundaries (and within) are support vectors\nMiddle margin is called the separating hyperplane\nthe hyperplane is determined by finding the optimal decision boundary while penalizing misclassifications.\nThe parameter C controls the trade-off between maximizing the margin and minimizing the classification error.\nGamma (or Sigma) low = big radius for classified points, high = low radius for classified points\n\n\n\n\n7.0.2 Application\n\nSVM\n\nThis article ‘Support vector machines for classification in remote sensing’(Pal and Mather, 2005) demonstrates the excellent performance of Support Vector Machines (SVM) in classification tasks within the field of remote sensing. The study presents findings from two experiments, comparing multi-class SVMs with maximum likelihood (ML) and artificial neural network (ANN) methods regarding classification accuracy. The experiments involve land cover classification using multispectral (Landsat-7 ETM+) and hyperspectral (DAIS) data in test areas located in eastern England and central Spain, respectively. Our results demonstrate that SVM achieves superior classification accuracy compared to both ML and ANN classifiers. Furthermore, SVM exhibits effectiveness with small training datasets and high-dimensional data.\n\nRM\n\nIncreasing Ntree typically improves model performance as it reduces overfitting and enhances generalization by aggregating predictions from multiple trees. However, increasing Ntree also increases computational costs, so there is a trade-off between model performance and computational resources. Typically, an appropriate value for Ntree can be selected through experimentation or empirical knowledge to achieve the desired classification or regression outcome. So, when I was learning about Random Forest, I felt a bit confused about this empirically chosen parameter, Ntree. The article ‘Random Forest in remote sensing: A review of applications and future directions’(Belgiu and Drăguţ, 2016) concludes, based on a review of other studies, that setting the default value of Ntree to 500 is acceptable when using the RF classifier on remotely sensed data.\n\n\n7.0.3 Reflection\nThis week, we focused on the application of machine learning in classification, which covered a lot of material similar to what we learned in casa006. Therefore, I found it relatively easy to understand. However, SVM was a method I wasn’t very familiar with, so I spent more time reading literature on this topic. I also discovered that SVM and Artificial Neural Networks are quite popular nowadays and yield accurate classification results. Nonetheless, I feel that machine learning encompasses a vast amount of knowledge, and I have only scratched the surface. I believe I need to dedicate more time to learning in order to deepen my understanding of machine learning."
  },
  {
    "objectID": "week7.html",
    "href": "week7.html",
    "title": "8  Week 7 - classification 2",
    "section": "",
    "text": "8.0.1 Summary\n\n8.0.1.1 Object based image analysis (OBIA)\n\nInstead of considering cells we consider shapes based on the similarity (homogeneity) or difference (heterogeneity) of the cells = superpixels\nSLIC (Simple Linear Iterative Clustering) Algorithm for Superpixel generation is the most common method\nThe principle involves segmenting remote sensing images into regions with similar characteristics and analyzing and classifying these regions based on their objects\n\n\n\n8.0.1.2 Sub pixel analysis\n\nthe information within a single pixel may be heterogeneous or contain multiple land cover types\nSub-pixel analysis allows for the detection and characterization of fine-scale features or land cover types that may not be distinguishable at the original pixel level.\ncalculate the proportions of different land cover types within a pixel\nSpectral Mixture Analysis (SMA) determines the proportion or abundance of landcover per pixel\n\n\n\n8.0.1.3 Accuracy assessment\n\n\n\n\nNew dataset to test the output with\nTrain / split the training data\nCross validation(best): It involves partitioning the dataset into subsets for training and testing, repeating this process multiple times to obtain reliable performance estimates, Leave OneOut Cross Validation(extreme version, only for smaller datasets): Each data point is held out once as the test set, and the model is trained on the remaining data points. This process is repeated for each data point in the dataset.\n\n\n\n\n\nKappa: Designed to express the accuracy of an image compared to the results by chance, Ranges from 0 to 1\nCalculate an error matrix\n\n\nSpatial cross validation\n\n\nspatially partition the folded data, folds are from cross validation\ndisjoint (no common boundary) using k -means clustering (number of points and a distance)\nsame as cross validation but with clustering to the folds\nstops our training data and testing data being near each other(this makes sure all the points (or pixels) we train the model with a far away from the points (or pixels) we test the model with)\n\n\n\n\n8.0.2 Application\nGiven that rural features like hedgerows are typically smaller than a single pixel, Thornton, Atkinson and Holland(2006) investigate the effectiveness of employing actual soft-classified high-resolution satellite sensor imagery to enhance the resolution of small rural land cover elements, such as trees and hedgerows, through sub-pixel mapping techniques.\nAccording to ‘Object-based detailed vegetation classification with airborne high spatial resolution remote sensing imagery’(Yu et al., 2006), the object-based classification method effectively addressed the issue of salt-and-pepper effects observed in traditional pixel-based classification outcomes. Testing conducted at Point Reyes National Seashore in Northern California demonstrated the efficacy of this classification technique in generating a thorough vegetation inventory.\n\n\n8.0.3 Reflection\nThis week, I’ve been learning about two new classification methods and accuracy assessment techniques. It took me a lot of time to understand their principles, and I still feel like I haven’t fully grasped them. However, accuracy assessment is really cool because now I can avoid getting stuck on which classification method is the best. Instead, I just need to apply various methods and then evaluate their accuracy to decide which one to use. While working on practical exercises, I realized that Google Earth Engine offers a plethora of possibilities. It’s truly an excellent platform."
  },
  {
    "objectID": "week8.html",
    "href": "week8.html",
    "title": "9  Week 8 - SAR",
    "section": "",
    "text": "9.0.1 Summary\n\n9.0.1.1 Synthetic Aperture Radar: SAR\n\nActive sensors\nHave surface texture data\nSee through weather and clouds\nDifferent wavelengths - different applications\nSAR utilizes radar beams to transmit and receive signals, measuring the echoes to obtain surface information. Unlike optical sensors, SAR can operate in any weather conditions and can work at night and under cloud cover.\nSAR can collect data in various polarization modes, including single polarization (HH or VV) dual polarization (HH/VV or HH/HV), and fully polarized. Different polarization modes provide different information suitable for various applications.\nVV = surface roughness VH = volume of surface Rough scattering (e.g. bare earth) = most sensitive to VV Volume scattering (e.g. leaves) = cross, VH or HV Double bounce (e.g. trees / buildings) = most sensitive to HH.\nA SAR signal has both amplitude (backscatter) and phase data\n\n\n\n9.0.1.2 INSAR & DInSAR\n\nInterferometric synthetic aperture radar (InSAR) techniques combine two or more SAR images over the same region to reveal surface topography or surface motion\nDifferential Interferometric Synthetic Aperture Radar (DInSAR): remove the effect of natural elevation\nSAR = active sensor, see through clouds, records energy reflected back\nInSAR = used for DEMs, converting phase different to relative height\nDInSAR = changes between two images in time. Looking at movement of land (uplift or sinking) with topography removed (using a DEM)\n\n\n\n9.0.1.3 Identifying change\n\nBlast Damage Assessment(t-test)\n\n\n\n\n9.0.2 Application\nIn the practical, based on Sentinel-1 SAR image, I devised a specialized change detection algorithm designed to perform a pixelwise t-test aimed at identifying alterations arising from the 2020 explosion incident in the port of Beirut.\nAccording to ‘LAND COVER MAPPING USING SENTINEL-1 SAR DATA’(Abdikan et al., 2016), this paper investigates the potential of using free-of-charge Sentinel-1 Synthetic Aperture Radar (SAR) imagery for urban land cover classification. They utilized dual-pol (VV+VH) Interferometric Wide swath mode (IW) data collected on September 16th, 2015, over the megacity of Istanbul, Turkey. The Support Vector Machines (SVM) method was employed for classification based on different combinations of VV and VH polarizations, and the results of each combination were evaluated. Their conclusion is that dual-polarimetric Sentinel-1 SAR data can be effectively utilized for generating accurate land cover classification maps, offering significant advantages for urban planning and management of large cities.\n\n\n9.0.3 Reflection\nThis week’s SAR section was already introduced during the PowerPoint presentation and group discussion in the second week. At that time, I realized the wide-ranging applications of SAR in remote sensing. The aspect of change detection is particularly intriguing, especially with this week’s practical on Blast Damage Assessment, along with Ollie’s lecture sharing about monitoring war destruction. It makes me appreciate the significant importance of SAR in post-war recovery and aiding victims. I am glad that the knowledge I’ve gained in remote sensing can contribute to reducing the impact of devastating conflicts."
  }
]